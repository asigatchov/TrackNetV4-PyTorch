import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import torch.nn.functional as F
import numpy as np
import os
import time
from pathlib import Path
import matplotlib.pyplot as plt
from tqdm import tqdm
from datetime import datetime

# å¯¼å…¥æ¨¡å‹å’Œæ•°æ®é›†
from tracknet import TrackNet, WeightedBCELoss, postprocess_heatmap
from dataset import BallTrackingDataset

# ======================== æ ¹æ®è®ºæ–‡çš„é…ç½®å‚æ•° ========================
TRAINING_CONFIG = {
    "dataset": {
        "base_dir": ".",
        "match_dir": "Dataset/Professional/match1",
        "input_height": 288,  # è®ºæ–‡ä¸­ä»640Ã—360æ”¹ä¸º512Ã—288
        "input_width": 512,
        "configs": {
            "3in3out": {
                "input_frames": 3,
                "output_frames": 3,  # MIMOè®¾è®¡: 3-in 3-out
                "normalize_coords": False,
                "normalize_pixels": False,
                "video_ext": ".mp4",
                "csv_suffix": "_ball.csv"
            },
            "3in1out": {
                "input_frames": 3,
                "output_frames": 1,  # MISOè®¾è®¡: 3-in 1-out
                "normalize_coords": False,
                "normalize_pixels": False,
                "video_ext": ".mp4",
                "csv_suffix": "_ball.csv"
            }
        }
    },
    "training": {
        "batch_size": 2,  # æ ¹æ®è®ºæ–‡å’ŒGPUå†…å­˜è°ƒæ•´
        "num_epochs": 30,  # è®ºæ–‡ä¸­ä½¿ç”¨30ä¸ªepochs
        "learning_rate": 1.0,  # è®ºæ–‡ä¸­ä½¿ç”¨1.0
        "weight_decay": 0.0,
        "train_ratio": 0.8,
        "val_ratio": 0.2
    },
    "model": {
        "heatmap_radius": 3,  # é«˜æ–¯çƒ­å›¾åŠå¾„
        "detection_threshold": 0.5,  # è®ºæ–‡ä¸­ä½¿ç”¨0.5é˜ˆå€¼
        "tolerance_pixels": 4  # è®ºæ–‡ä¸­ä½¿ç”¨4åƒç´ å®¹å¿åº¦
    },
    "optimization": {
        "optimizer": "Adadelta",  # è®ºæ–‡æŒ‡å®šä½¿ç”¨Adadelta
        "scheduler": {
            "type": "ReduceLROnPlateau",
            "mode": "min",
            "factor": 0.5,
            "patience": 5,
            "verbose": True
        }
    },
    "early_stopping": {
        "enabled": True,
        "patience": 15,
        "min_delta": 1e-4
    },
    "logging": {
        "save_interval": 10,
        "print_interval": 10,
        "plot_interval": 5
    },
    "paths": {
        "save_dir": "checkpoints",
        "log_dir": "logs"
    }
}


# ================================================================


def create_gaussian_heatmap(x, y, visibility, height, width, radius=3):
    """æ ¹æ®è®ºæ–‡åˆ›å»ºé«˜æ–¯çƒ­å›¾ - 2D Gaussian distribution"""
    heatmap = torch.zeros(height, width)

    if visibility < 0.5:  # çƒä¸å¯è§
        return heatmap

    # è½¬æ¢å½’ä¸€åŒ–åæ ‡åˆ°åƒç´ åæ ‡
    x_pixel = int(x * width)
    y_pixel = int(y * height)

    # ç¡®ä¿åæ ‡åœ¨è¾¹ç•Œå†…
    x_pixel = max(0, min(width - 1, x_pixel))
    y_pixel = max(0, min(height - 1, y_pixel))

    # åˆ›å»ºé«˜æ–¯åˆ†å¸ƒ - è®ºæ–‡ä¸­ä½¿ç”¨çš„amplified 2D Gaussian
    y_coords, x_coords = torch.meshgrid(torch.arange(height), torch.arange(width), indexing='ij')

    # è®¡ç®—è·ç¦»å¹³æ–¹
    dist_sq = (x_coords - x_pixel) ** 2 + (y_coords - y_pixel) ** 2

    # ç”Ÿæˆé«˜æ–¯çƒ­å›¾
    heatmap = torch.exp(-dist_sq / (2 * radius ** 2))

    # é˜ˆå€¼å¤„ç† - è®ºæ–‡ä¸­æåˆ°çš„å¤„ç†æ–¹å¼
    heatmap[heatmap < 0.01] = 0

    return heatmap


def collate_fn(batch):
    """æ ¹æ®è®ºæ–‡è¦æ±‚çš„æ•°æ®å¤„ç†: 720Ã—1280 -> 288Ã—512"""
    config = TRAINING_CONFIG["dataset"]
    target_height = config["input_height"]
    target_width = config["input_width"]

    frames_list = []
    heatmaps_list = []

    for frames, labels in batch:
        # å¤„ç†è¾“å…¥å¸§ï¼šè°ƒæ•´å°ºå¯¸åˆ°è®ºæ–‡è¦æ±‚çš„512Ã—288
        frames = frames.unsqueeze(0)  # [1, 9, H, W]
        frames_resized = F.interpolate(frames, size=(target_height, target_width),
                                       mode='bilinear', align_corners=False)
        frames_resized = frames_resized.squeeze(0)  # [9, 288, 512]
        frames_list.append(frames_resized)

        # å¤„ç†æ ‡ç­¾ï¼šä»åæ ‡å­—å…¸è½¬æ¢ä¸ºçƒ­å›¾
        num_frames = len(labels)
        heatmaps = torch.zeros(num_frames, target_height, target_width)

        for i, label_dict in enumerate(labels):
            if isinstance(label_dict, dict):
                x = label_dict['x'].item()
                y = label_dict['y'].item()
                visibility = label_dict['visibility'].item()

                heatmap = create_gaussian_heatmap(x, y, visibility,
                                                  target_height, target_width,
                                                  TRAINING_CONFIG["model"]["heatmap_radius"])
                heatmaps[i] = heatmap

        heatmaps_list.append(heatmaps)

    batch_frames = torch.stack(frames_list)
    batch_heatmaps = torch.stack(heatmaps_list)

    return batch_frames, batch_heatmaps


class TrackNetV2Trainer:
    def __init__(self):
        self.config = TRAINING_CONFIG
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆ›å»ºä¿å­˜ç›®å½•
        Path(self.config["paths"]["save_dir"]).mkdir(exist_ok=True)
        Path(self.config["paths"]["log_dir"]).mkdir(exist_ok=True)

        # è®­ç»ƒè®°å½•
        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')
        self.early_stop_counter = 0

    def print_banner(self):
        """æ‰“å°ç¨‹åºæ ‡é¢˜"""
        print("=" * 60)
        print("        TrackNetV2 ç¾½æ¯›çƒè¿½è¸ªè®­ç»ƒç¨‹åº")
        print("        åŸºäºè®ºæ–‡: TrackNetV2: Efficient Shuttlecock Tracking Network")
        print("=" * 60)
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ•°æ®ç›®å½•: {self.config['dataset']['match_dir']}")
        print(f"è¾“å…¥å°ºå¯¸: {self.config['dataset']['input_width']}Ã—{self.config['dataset']['input_height']}")
        print(f"ä¼˜åŒ–å™¨: {self.config['optimization']['optimizer']}")
        print(f"å­¦ä¹ ç‡: {self.config['training']['learning_rate']}")
        print(f"è®­ç»ƒè½®æ•°: {self.config['training']['num_epochs']}")
        print()

    def select_model_config(self):
        """é€‰æ‹©æ¨¡å‹é…ç½®"""
        print("è¯·é€‰æ‹©TrackNetV2é…ç½®:")
        print("1. 3-in-3-out (MIMO): æ›´é«˜ååé‡, è®ºæ–‡æ¨èé…ç½®")
        print("2. 3-in-1-out (MISO): ä¼ ç»Ÿé…ç½®")

        while True:
            choice = input("è¯·è¾“å…¥é€‰æ‹© (1-2): ").strip()
            if choice == "1":
                return "3in3out"
            elif choice == "2":
                return "3in1out"
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥!")

    def setup_model_and_optimizer(self, config_name):
        """æ ¹æ®è®ºæ–‡é…ç½®è®¾ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨"""
        dataset_config = self.config["dataset"]["configs"][config_name]

        # åˆ›å»ºTrackNetæ¨¡å‹
        self.model = TrackNet()

        # æ ¹æ®è¾“å‡ºå¸§æ•°è°ƒæ•´æœ€åä¸€å±‚ - è®ºæ–‡ä¸­çš„MIMOè®¾è®¡
        if dataset_config['output_frames'] != 3:
            self.model.conv2d_18 = nn.Conv2d(64, dataset_config['output_frames'], 1)

        self.model = self.model.to(self.device)

        # è®ºæ–‡ä¸­çš„æŸå¤±å‡½æ•°
        self.criterion = WeightedBCELoss()

        # è®ºæ–‡ä¸­æŒ‡å®šçš„ä¼˜åŒ–å™¨
        self.optimizer = optim.Adadelta(
            self.model.parameters(),
            lr=self.config['training']['learning_rate'],
            weight_decay=self.config['training']['weight_decay']
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_config = self.config['optimization']['scheduler']
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode=scheduler_config['mode'],
            factor=scheduler_config['factor'],
            patience=scheduler_config['patience'],
            verbose=scheduler_config['verbose']
        )

        print(f"âœ“ æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"âœ“ é…ç½®: {config_name} ({dataset_config['input_frames']}è¿›{dataset_config['output_frames']}å‡º)")

    def setup_data_loaders(self, config_name):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        dataset_config = self.config["dataset"]["configs"][config_name]
        match_dir = Path(self.config["dataset"]["base_dir"]) / self.config["dataset"]["match_dir"]

        print(f"\nè®¾ç½®æ•°æ®åŠ è½½å™¨...")
        print(f"æ•°æ®ç›®å½•: {match_dir}")

        try:
            dataset = BallTrackingDataset(str(match_dir), config=dataset_config)
            print(f"âœ“ æ•°æ®é›†å¤§å°: {len(dataset)}")
        except Exception as e:
            print(f"âœ— åˆ›å»ºæ•°æ®é›†å¤±è´¥: {e}")
            return False

        # åˆ†å‰²æ•°æ®é›†
        train_size = int(self.config['training']['train_ratio'] * len(dataset))
        val_size = len(dataset) - train_size

        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
        print(f"âœ“ è®­ç»ƒé›†: {train_size}, éªŒè¯é›†: {val_size}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['training']['batch_size'],
            shuffle=True,
            num_workers=2,
            collate_fn=collate_fn,
            pin_memory=True if self.device.type == 'cuda' else False
        )

        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['training']['batch_size'],
            shuffle=False,
            num_workers=2,
            collate_fn=collate_fn,
            pin_memory=True if self.device.type == 'cuda' else False
        )

        print(f"âœ“ è®­ç»ƒæ‰¹æ¬¡: {len(self.train_loader)}, éªŒè¯æ‰¹æ¬¡: {len(self.val_loader)}")
        return True

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(self.train_loader)

        progress_bar = tqdm(self.train_loader, desc=f"Epoch {epoch + 1}/{self.config['training']['num_epochs']}")

        for batch_idx, (inputs, targets) in enumerate(progress_bar):
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)

            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()

            # æ›´æ–°è¿›åº¦æ¡
            if batch_idx % self.config['logging']['print_interval'] == 0:
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.6f}',
                    'Avg': f'{total_loss / (batch_idx + 1):.6f}',
                    'LR': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
                })

        avg_loss = total_loss / num_batches
        self.train_losses.append(avg_loss)
        return avg_loss

    def validate_epoch(self):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        num_batches = len(self.val_loader)

        with torch.no_grad():
            for inputs, targets in tqdm(self.val_loader, desc="éªŒè¯ä¸­"):
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets)
                total_loss += loss.item()

        avg_loss = total_loss / num_batches
        self.val_losses.append(avg_loss)
        return avg_loss

    def save_checkpoint(self, epoch, config_name, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'best_val_loss': self.best_val_loss,
            'config_name': config_name,
            'training_config': self.config
        }

        save_dir = Path(self.config["paths"]["save_dir"])

        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = save_dir / f'latest_{config_name}.pth'
        torch.save(checkpoint, latest_path)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = save_dir / f'best_{config_name}.pth'
            torch.save(checkpoint, best_path)
            print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹! éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")

        # å®šæœŸä¿å­˜
        if (epoch + 1) % self.config['logging']['save_interval'] == 0:
            epoch_path = save_dir / f'checkpoint_{config_name}_epoch_{epoch + 1}.pth'
            torch.save(checkpoint, epoch_path)

    def plot_losses(self, config_name):
        """ç»˜åˆ¶æŸå¤±æ›²çº¿"""
        if len(self.train_losses) < 2:
            return

        plt.figure(figsize=(12, 4))

        plt.subplot(1, 2, 1)
        plt.plot(self.train_losses, label='è®­ç»ƒæŸå¤±', color='blue', linewidth=2)
        plt.plot(self.val_losses, label='éªŒè¯æŸå¤±', color='red', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Weighted BCE Loss')
        plt.title(f'TrackNetV2 è®­ç»ƒæ›²çº¿ - {config_name}')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.subplot(1, 2, 2)
        if len(self.train_losses) > 10:
            recent_epochs = min(20, len(self.train_losses))
            epochs = range(len(self.train_losses) - recent_epochs, len(self.train_losses))
            plt.plot(epochs, self.train_losses[-recent_epochs:],
                     label=f'è®­ç»ƒæŸå¤± (æœ€è¿‘{recent_epochs}è½®)', color='blue', linewidth=2)
            plt.plot(epochs, self.val_losses[-recent_epochs:],
                     label=f'éªŒè¯æŸå¤± (æœ€è¿‘{recent_epochs}è½®)', color='red', linewidth=2)
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.title('æœ€è¿‘è®­ç»ƒè¿›åº¦')
            plt.legend()
            plt.grid(True, alpha=0.3)

        plt.tight_layout()
        log_dir = Path(self.config["paths"]["log_dir"])
        plt.savefig(log_dir / f'loss_curves_{config_name}.png', dpi=300, bbox_inches='tight')
        plt.close()

    def train_model(self, config_name):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ TrackNetV2 - {config_name}")
        print("-" * 60)

        start_time = time.time()

        for epoch in range(self.config['training']['num_epochs']):
            epoch_start = time.time()

            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)

            # éªŒè¯
            val_loss = self.validate_epoch()

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_loss)

            epoch_time = time.time() - epoch_start
            current_lr = self.optimizer.param_groups[0]['lr']

            # æ‰“å°è¿›åº¦
            print(f"Epoch {epoch + 1:3d}/{self.config['training']['num_epochs']}")
            print(f"  ğŸ“ˆ è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            print(f"  ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"  â±ï¸  ç”¨æ—¶: {epoch_time:.1f}s, å­¦ä¹ ç‡: {current_lr:.2e}")

            # æ£€æŸ¥æœ€ä½³æ¨¡å‹
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss
                self.early_stop_counter = 0
            else:
                self.early_stop_counter += 1

            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch, config_name, is_best)

            # ç»˜åˆ¶æŸå¤±æ›²çº¿
            if (epoch + 1) % self.config['logging']['plot_interval'] == 0:
                self.plot_losses(config_name)

            # æ—©åœæ£€æŸ¥
            if (self.config['early_stopping']['enabled'] and
                    self.early_stop_counter >= self.config['early_stopping']['patience']):
                print(f"â° æ—©åœè§¦å‘! åœ¨ç¬¬ {epoch + 1} è½®åœæ­¢è®­ç»ƒ")
                break

            print("-" * 60)

        total_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ æ€»ç”¨æ—¶: {total_time / 3600:.2f} å°æ—¶")
        print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")

        # æœ€ç»ˆä¿å­˜
        self.save_checkpoint(epoch, config_name, False)
        self.plot_losses(config_name)

    def test_data_loading(self, config_name):
        """æµ‹è¯•æ•°æ®åŠ è½½"""
        print(f"\nğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½å’Œæ¨¡å‹å‰å‘ä¼ æ’­...")

        try:
            for inputs, targets in self.train_loader:
                print(f"âœ“ è¾“å…¥å½¢çŠ¶: {inputs.shape}")
                print(f"âœ“ ç›®æ ‡å½¢çŠ¶: {targets.shape}")
                print(f"âœ“ è¾“å…¥èŒƒå›´: [{inputs.min():.3f}, {inputs.max():.3f}]")
                print(f"âœ“ ç›®æ ‡èŒƒå›´: [{targets.min():.3f}, {targets.max():.3f}]")

                # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)

                with torch.no_grad():
                    outputs = self.model(inputs)
                    loss = self.criterion(outputs, targets)

                print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
                print(f"âœ“ æŸå¤±å€¼: {loss.item():.6f}")
                print("âœ… æ•°æ®åŠ è½½å’Œæ¨¡å‹æµ‹è¯•æˆåŠŸ!")
                return True

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            return False

    def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        self.print_banner()

        # 1. é€‰æ‹©æ¨¡å‹é…ç½®
        config_name = self.select_model_config()
        print(f"âœ… é€‰æ‹©é…ç½®: {config_name}")

        # 2. è®¾ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨
        self.setup_model_and_optimizer(config_name)

        # 3. è®¾ç½®æ•°æ®åŠ è½½å™¨
        if not self.setup_data_loaders(config_name):
            print("âŒ æ•°æ®åŠ è½½å™¨è®¾ç½®å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return

        # 4. æµ‹è¯•æ•°æ®åŠ è½½
        if not self.test_data_loading(config_name):
            print("âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†!")
            return

        # 5. å¼€å§‹è®­ç»ƒ
        print(f"\nğŸ¯ è®ºæ–‡é…ç½®æ€»ç»“:")
        print(f"   - è¾“å…¥å°ºå¯¸: 512Ã—288Ã—9")
        print(f"   - è¾“å‡º: {config_name}")
        print(f"   - æŸå¤±å‡½æ•°: Weighted BCE (è®ºæ–‡å…¬å¼)")
        print(f"   - ä¼˜åŒ–å™¨: Adadelta (lr=1.0)")
        print(f"   - è½®æ•°: 30")

        input("\næŒ‰å›è½¦é”®å¼€å§‹è®­ç»ƒ...")
        self.train_model(config_name)

        print("\nğŸ è®­ç»ƒç¨‹åºç»“æŸ!")


if __name__ == "__main__":
    trainer = TrackNetV2Trainer()
    trainer.run()
