#!/usr/bin/env python3
"""
TrackNetV2 ç¾½æ¯›çƒè¿½è¸ªç½‘ç»œè®­ç»ƒè„šæœ¬ - æ”¯æŒå¼ºåˆ¶ç»“æŸæ—¶è‡ªåŠ¨ä¿å­˜
- æ”¯æŒCUDA/MPS/CPUè‡ªåŠ¨é€‰æ‹©
- æ”¯æŒä»å¤´è®­ç»ƒå’Œæ–­ç‚¹ç»­è®­
- MIMOè®¾è®¡ï¼Œæ¯epochè‡ªåŠ¨ä¿å­˜
- å¼ºåˆ¶ç»“æŸæ—¶è‡ªåŠ¨ä¿å­˜å½“å‰æ¨¡å‹
"""

import argparse
import json
import logging
import time
import signal
import sys
import atexit
from pathlib import Path
from typing import Dict, Tuple, Optional

import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from tqdm import tqdm

from dataset_controller.ball_tracking_data_reader import BallTrackingDataset
from tracknet import TrackNetV4, WeightedBCELoss

# å…¨å±€å˜é‡ï¼Œç”¨äºä¿¡å·å¤„ç†
_trainer_instance = None

# é»˜è®¤é…ç½®
DEFAULT_CONFIG = {
    "batch_size": 2,
    "num_epochs": 30,
    "learning_rate": 1.0,
    "weight_decay": 0.0,
    "gradient_clip": 1.0,
    "input_height": 288,
    "input_width": 512,
    "heatmap_radius": 3,
    "detection_threshold": 0.5,
    "distance_threshold": 4,
    "scheduler_factor": 0.5,
    "scheduler_patience": 8,
    "early_stop_patience": 15,
    "train_split": 0.8,
    "save_interval": 1,  # æ¯epochä¿å­˜
}

DATASET_CONFIG = {
    "input_frames": 3,
    "output_frames": 3,  # MIMOè®¾è®¡
    "normalize_coords": True,
    "normalize_pixels": True,
    "video_ext": ".mp4",
    "csv_suffix": "_ball.csv"
}


def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å‡½æ•° - å¤„ç†Ctrl+Cç­‰å¼ºåˆ¶ç»“æŸä¿¡å·"""
    global _trainer_instance

    signal_names = {
        signal.SIGINT: "SIGINT (Ctrl+C)",
        signal.SIGTERM: "SIGTERM"
    }

    signal_name = signal_names.get(signum, f"Signal {signum}")
    print(f"\nâš ï¸  æ”¶åˆ°{signal_name}ä¿¡å·ï¼Œæ­£åœ¨å®‰å…¨ä¿å­˜æ¨¡å‹...")

    if _trainer_instance is not None:
        try:
            # ä¿å­˜ç´§æ€¥æ£€æŸ¥ç‚¹
            emergency_path = _trainer_instance.save_dir / f'emergency_save_epoch_{_trainer_instance.current_epoch:03d}.pth'
            _trainer_instance.save_emergency_checkpoint(emergency_path)
            print(f"âœ“ ç´§æ€¥ä¿å­˜å®Œæˆ: {emergency_path}")
        except Exception as e:
            print(f"âŒ ç´§æ€¥ä¿å­˜å¤±è´¥: {e}")

    print("ğŸ”„ è¿›ç¨‹å®‰å…¨é€€å‡º")
    sys.exit(0)


def cleanup_on_exit():
    """ç¨‹åºé€€å‡ºæ—¶çš„æ¸…ç†å‡½æ•°"""
    global _trainer_instance
    if _trainer_instance is not None and hasattr(_trainer_instance, 'training_in_progress'):
        if _trainer_instance.training_in_progress:
            print("\nğŸ”„ ç¨‹åºæ­£å¸¸é€€å‡ºï¼Œæ‰§è¡Œæœ€åä¿å­˜...")
            try:
                exit_path = _trainer_instance.save_dir / f'exit_save_epoch_{_trainer_instance.current_epoch:03d}.pth'
                _trainer_instance.save_emergency_checkpoint(exit_path)
                print(f"âœ“ é€€å‡ºä¿å­˜å®Œæˆ: {exit_path}")
            except Exception as e:
                print(f"âŒ é€€å‡ºä¿å­˜å¤±è´¥: {e}")


def get_device_and_config():
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        config = {"num_workers": 4, "pin_memory": True, "persistent_workers": True}
        print(f"âœ“ CUDA: {torch.cuda.get_device_name()}")

        # æ ¹æ®æ˜¾å­˜è°ƒæ•´æ‰¹æ¬¡
        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        if memory_gb < 8:
            config["batch_multiplier"] = 0.5

        # CUDAä¼˜åŒ–
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = torch.device('mps')
        config = {"num_workers": 2, "pin_memory": False, "persistent_workers": False}
        print("âœ“ MPS: Apple Silicon")

    else:
        device = torch.device('cpu')
        config = {"num_workers": 4, "pin_memory": False, "persistent_workers": True}
        print("âš ï¸ CPUæ¨¡å¼")

    return device, config


def init_weights(m):
    """æƒé‡åˆå§‹åŒ– - æŒ‰è®ºæ–‡è¦æ±‚ä½¿ç”¨uniform"""
    if isinstance(m, nn.Conv2d):
        nn.init.uniform_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.BatchNorm2d):
        nn.init.ones_(m.weight)
        nn.init.zeros_(m.bias)


def create_gaussian_heatmap(x, y, visibility, height, width, radius=3.0):
    """ç”Ÿæˆé«˜æ–¯çƒ­å›¾"""
    heatmap = torch.zeros(height, width)
    if visibility < 0.5:
        return heatmap

    x_pixel = max(0, min(width - 1, int(x * width)))
    y_pixel = max(0, min(height - 1, int(y * height)))

    kernel_size = int(3 * radius)
    x_min, x_max = max(0, x_pixel - kernel_size), min(width, x_pixel + kernel_size + 1)
    y_min, y_max = max(0, y_pixel - kernel_size), min(height, y_pixel + kernel_size + 1)

    y_coords, x_coords = torch.meshgrid(
        torch.arange(y_min, y_max),
        torch.arange(x_min, x_max),
        indexing='ij'
    )

    dist_sq = (x_coords - x_pixel) ** 2 + (y_coords - y_pixel) ** 2
    gaussian_values = torch.exp(-dist_sq / (2 * radius ** 2))
    gaussian_values[gaussian_values < 0.01] = 0

    heatmap[y_min:y_max, x_min:x_max] = gaussian_values
    return heatmap


def collate_fn(batch):
    """æ•°æ®æ‰¹å¤„ç†"""
    config = DEFAULT_CONFIG
    frames_list, heatmaps_list = [], []

    for frames, labels in batch:
        # è°ƒæ•´è¾“å…¥å°ºå¯¸
        frames = F.interpolate(
            frames.unsqueeze(0),
            size=(config["input_height"], config["input_width"]),
            mode='bilinear',
            align_corners=False
        ).squeeze(0)
        frames_list.append(frames)

        # ç”Ÿæˆçƒ­å›¾
        heatmaps = torch.zeros(len(labels), config["input_height"], config["input_width"])
        for i, label_dict in enumerate(labels):
            if isinstance(label_dict, dict):
                heatmap = create_gaussian_heatmap(
                    label_dict['x'].item(),
                    label_dict['y'].item(),
                    label_dict['visibility'].item(),
                    config["input_height"],
                    config["input_width"],
                    config["heatmap_radius"]
                )
                heatmaps[i] = heatmap
        heatmaps_list.append(heatmaps)

    return torch.stack(frames_list), torch.stack(heatmaps_list)


def load_dataset(data_dir):
    """åŠ è½½æ•°æ®é›†"""
    data_dir = Path(data_dir)
    match_dirs = sorted([d for d in data_dir.iterdir() if d.is_dir() and d.name.startswith('match')])

    if not match_dirs:
        raise ValueError(f"æœªæ‰¾åˆ°matchæ–‡ä»¶å¤¹: {data_dir}")

    combined_dataset = None
    for match_dir in match_dirs:
        try:
            dataset = BallTrackingDataset(str(match_dir), config=DATASET_CONFIG)
            if len(dataset) > 0:
                combined_dataset = dataset if combined_dataset is None else combined_dataset + dataset
                print(f"âœ“ {match_dir.name}: {len(dataset)} æ ·æœ¬")
        except Exception as e:
            print(f"âœ— {match_dir.name} åŠ è½½å¤±è´¥: {e}")

    if combined_dataset is None:
        raise ValueError("æ— å¯ç”¨æ•°æ®é›†")

    print(f"æ€»è®¡: {len(combined_dataset)} æ ·æœ¬")
    return combined_dataset


class Trainer:
    def __init__(self, args, device, device_config):
        self.args = args
        self.device = device
        self.device_config = device_config

        # åˆ›å»ºç›®å½•
        self.save_dir = Path(args.save_dir)
        self.save_dir.mkdir(exist_ok=True)

        # è®­ç»ƒçŠ¶æ€è¿½è¸ª
        self.current_epoch = 0
        self.current_batch = 0
        self.training_in_progress = False

        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.save_dir / 'training.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

        # è®­ç»ƒçŠ¶æ€
        self.start_epoch = 0
        self.best_loss = float('inf')
        self.train_losses = []
        self.val_losses = []
        self.learning_rates = []

        # åˆå§‹åŒ–æ¨¡å‹
        self.setup_model()

        # åŠ è½½æ£€æŸ¥ç‚¹
        if args.resume:
            self.load_checkpoint(args.resume)

    def setup_model(self):
        """åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨"""
        self.model = TrackNetV4()
        # MIMOè¾“å‡º
        self.model.conv2d_18 = nn.Conv2d(64, DATASET_CONFIG['output_frames'], 1)

        # æƒé‡åˆå§‹åŒ–
        self.model.apply(init_weights)
        self.model = self.model.to(self.device)

        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = WeightedBCELoss()
        self.optimizer = optim.Adadelta(
            self.model.parameters(),
            lr=self.args.lr,
            weight_decay=self.args.weight_decay
        )
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            factor=DEFAULT_CONFIG["scheduler_factor"],
            patience=DEFAULT_CONFIG["scheduler_patience"]
        )

        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in self.model.parameters())
        self.logger.info(f"æ¨¡å‹å‚æ•°: {total_params:,}")

    def save_checkpoint(self, epoch, is_best=False, checkpoint_type="regular"):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'current_batch': self.current_batch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_loss': self.best_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'learning_rates': self.learning_rates,
            'config': vars(self.args),
            'checkpoint_type': checkpoint_type,
            'save_time': time.time()
        }

        # ä¿å­˜æœ€æ–°
        torch.save(checkpoint, self.save_dir / f'epoch_{epoch:03d}.pth')
        torch.save(checkpoint, self.save_dir / 'latest.pth')

        # ä¿å­˜æœ€ä½³
        if is_best:
            torch.save(checkpoint, self.save_dir / 'best.pth')
            self.logger.info(f"âœ“ æœ€ä½³æ¨¡å‹ Epoch {epoch}: {self.best_loss:.6f}")

    def save_emergency_checkpoint(self, save_path):
        """ä¿å­˜ç´§æ€¥æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'current_batch': self.current_batch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_loss': self.best_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'learning_rates': self.learning_rates,
            'config': vars(self.args),
            'checkpoint_type': "emergency",
            'save_time': time.time()
        }

        torch.save(checkpoint, save_path)
        # åŒæ—¶ä¿å­˜ä¸ºæœ€æ–°æ£€æŸ¥ç‚¹
        torch.save(checkpoint, self.save_dir / 'latest.pth')

    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint_path = Path(checkpoint_path)
        if not checkpoint_path.exists():
            self.logger.warning(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            return

        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        self.start_epoch = checkpoint['epoch'] + 1
        self.best_loss = checkpoint['best_loss']
        self.train_losses = checkpoint['train_losses']
        self.val_losses = checkpoint['val_losses']
        self.learning_rates = checkpoint['learning_rates']

        # å¦‚æœæ˜¯ç´§æ€¥ä¿å­˜çš„æ£€æŸ¥ç‚¹ï¼Œæ˜¾ç¤ºç‰¹æ®Šä¿¡æ¯
        checkpoint_type = checkpoint.get('checkpoint_type', 'regular')
        if checkpoint_type == 'emergency':
            self.logger.info(f"âœ“ ä»ç´§æ€¥ä¿å­˜çš„æ£€æŸ¥ç‚¹æ¢å¤: Epoch {self.start_epoch}")
        else:
            self.logger.info(f"âœ“ ä»Epoch {self.start_epoch}ç»§ç»­è®­ç»ƒ")

    def train_epoch(self, epoch, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        self.current_epoch = epoch

        pbar = tqdm(train_loader, desc=f"Epoch {epoch:03d}")
        for batch_idx, (inputs, targets) in enumerate(pbar):
            self.current_batch = batch_idx

            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            if self.args.grad_clip > 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.grad_clip)

            self.optimizer.step()

            total_loss += loss.item()
            pbar.set_postfix({'Loss': f'{loss.item():.6f}'})

        return total_loss / len(train_loader)

    def validate(self, val_loader):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0.0

        with torch.no_grad():
            for inputs, targets in tqdm(val_loader, desc="éªŒè¯"):
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets)
                total_loss += loss.item()

        return total_loss / len(val_loader)

    def plot_curves(self, epoch):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if len(self.train_losses) < 2:
            return

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

        epochs = range(1, len(self.train_losses) + 1)

        # æŸå¤±æ›²çº¿
        ax1.plot(epochs, self.train_losses, 'b-', label='è®­ç»ƒ', linewidth=2)
        ax1.plot(epochs, self.val_losses, 'r-', label='éªŒè¯', linewidth=2)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('æŸå¤±æ›²çº¿')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # å­¦ä¹ ç‡æ›²çº¿
        ax2.plot(epochs, self.learning_rates, 'g-', linewidth=2)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Learning Rate')
        ax2.set_title('å­¦ä¹ ç‡å˜åŒ–')
        ax2.set_yscale('log')
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.save_dir / f'curves_epoch_{epoch:03d}.png', dpi=150, bbox_inches='tight')
        plt.close()

    def train(self, train_dataset, val_dataset):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        # æ ‡è®°è®­ç»ƒå¼€å§‹
        self.training_in_progress = True

        # æ•°æ®åŠ è½½å™¨
        data_kwargs = {
            'batch_size': self.args.batch_size,
            'num_workers': self.device_config['num_workers'],
            'pin_memory': self.device_config['pin_memory'],
            'persistent_workers': self.device_config['persistent_workers'],
            'collate_fn': collate_fn,
            'drop_last': True
        }

        train_loader = DataLoader(train_dataset, shuffle=False, **data_kwargs)
        val_loader = DataLoader(val_dataset, shuffle=False, **data_kwargs)

        self.logger.info(f"è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")
        self.logger.info(f"è®¾å¤‡: {self.device}")
        self.logger.info("âš ï¸  æŒ‰Ctrl+Cå¯å®‰å…¨åœæ­¢è®­ç»ƒå¹¶è‡ªåŠ¨ä¿å­˜æ¨¡å‹")

        # æ—©åœè®¡æ•°å™¨
        patience_counter = 0
        start_time = time.time()

        try:
            for epoch in range(self.start_epoch, self.args.epochs):
                # è®­ç»ƒå’ŒéªŒè¯
                train_loss = self.train_epoch(epoch, train_loader)
                val_loss = self.validate(val_loader)

                # æ›´æ–°å­¦ä¹ ç‡
                self.scheduler.step(val_loss)
                current_lr = self.optimizer.param_groups[0]['lr']

                # è®°å½•æŒ‡æ ‡
                self.train_losses.append(train_loss)
                self.val_losses.append(val_loss)
                self.learning_rates.append(current_lr)

                # æ£€æŸ¥æœ€ä½³æ¨¡å‹
                is_best = val_loss < self.best_loss
                if is_best:
                    self.best_loss = val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1

                # è®°å½•è¿›åº¦
                self.logger.info(
                    f"Epoch {epoch:03d}: è®­ç»ƒ={train_loss:.6f}, "
                    f"éªŒè¯={val_loss:.6f}, LR={current_lr:.2e}"
                    f"{' [BEST]' if is_best else ''}"
                )

                # ä¿å­˜æ£€æŸ¥ç‚¹å’Œå›¾è¡¨
                if epoch % self.args.save_interval == 0 or is_best:
                    self.save_checkpoint(epoch, is_best)
                    self.plot_curves(epoch)

                # æ—©åœæ£€æŸ¥
                if patience_counter >= DEFAULT_CONFIG["early_stop_patience"]:
                    self.logger.info(f"æ—©åœè§¦å‘ï¼ŒEpoch {epoch}")
                    break

        except KeyboardInterrupt:
            self.logger.info("\nâš ï¸  æ”¶åˆ°é”®ç›˜ä¸­æ–­ä¿¡å·")
            # è¿™é‡Œçš„ä¿å­˜ç”±ä¿¡å·å¤„ç†å™¨å¤„ç†

        except Exception as e:
            self.logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            # ä¿å­˜å¼‚å¸¸æ—¶çš„æ£€æŸ¥ç‚¹
            try:
                exception_path = self.save_dir / f'exception_save_epoch_{self.current_epoch:03d}.pth'
                self.save_emergency_checkpoint(exception_path)
                self.logger.info(f"âœ“ å¼‚å¸¸ä¿å­˜å®Œæˆ: {exception_path}")
            except Exception as save_error:
                self.logger.error(f"âŒ å¼‚å¸¸ä¿å­˜å¤±è´¥: {save_error}")
            raise

        finally:
            # æ ‡è®°è®­ç»ƒç»“æŸ
            self.training_in_progress = False

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        self.logger.info("=" * 50)
        self.logger.info(f"è®­ç»ƒå®Œæˆ! ç”¨æ—¶: {total_time / 3600:.2f}å°æ—¶")
        self.logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.6f}")
        self.logger.info("=" * 50)


def main():
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # ç»ˆæ­¢ä¿¡å·

    # æ³¨å†Œé€€å‡ºæ¸…ç†å‡½æ•°
    atexit.register(cleanup_on_exit)

    parser = argparse.ArgumentParser(description='TrackNetV2 ç¾½æ¯›çƒè¿½è¸ªè®­ç»ƒ')

    # æ•°æ®å‚æ•°
    parser.add_argument('--data_dir', type=str, required=True, help='æ•°æ®é›†ç›®å½•')
    parser.add_argument('--save_dir', type=str, default='checkpoints', help='ä¿å­˜ç›®å½•')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=2, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=30, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1.0, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=0.0, help='æƒé‡è¡°å‡')
    parser.add_argument('--grad_clip', type=float, default=1.0, help='æ¢¯åº¦è£å‰ª')
    parser.add_argument('--save_interval', type=int, default=1, help='ä¿å­˜é—´éš”')

    # ç»­è®­å‚æ•°
    parser.add_argument('--resume', type=str, help='ç»§ç»­è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')

    args = parser.parse_args()

    # è·å–è®¾å¤‡
    device, device_config = get_device_and_config()

    # æ ¹æ®è®¾å¤‡è°ƒæ•´æ‰¹æ¬¡å¤§å°
    if 'batch_multiplier' in device_config:
        args.batch_size = max(1, int(args.batch_size * device_config['batch_multiplier']))
        print(f"æ‰¹æ¬¡å¤§å°è°ƒæ•´ä¸º: {args.batch_size}")

    try:
        # åŠ è½½æ•°æ®é›†
        print(f"\nåŠ è½½æ•°æ®é›†: {args.data_dir}")
        full_dataset = load_dataset(args.data_dir)

        # åˆ†å‰²æ•°æ®é›†
        total_size = len(full_dataset)
        train_size = int(DEFAULT_CONFIG['train_split'] * total_size)
        indices = torch.randperm(total_size).tolist()

        train_dataset = Subset(full_dataset, indices[:train_size])
        val_dataset = Subset(full_dataset, indices[train_size:])

        print(f"è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")

        # è®¾ç½®å…¨å±€trainerå®ä¾‹ï¼ˆç”¨äºä¿¡å·å¤„ç†ï¼‰
        global _trainer_instance
        trainer = Trainer(args, device, device_config)
        _trainer_instance = trainer

        # å¼€å§‹è®­ç»ƒ
        trainer.train(train_dataset, val_dataset)

    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

    """
    æ–°æ¨¡å‹è®­ç»ƒï¼špython train.py --data_dir Dataset/Professional --save_dir checkpoints
    ç»§ç»­è®­ç»ƒï¼špython train.py --data_dir Dataset/Professional --resume checkpoints/latest.pth
    å…¨å‚æ•°è®­ç»ƒï¼špython train.py --data_dir Dataset/Professional --save_dir checkpoints --batch_size 2 --epochs 30 --lr 1.0 --weight_decay 0.0 --grad_clip 1.0 --save_interval 1
    
    å¼ºåˆ¶ç»“æŸæ—¶ä¼šè‡ªåŠ¨ä¿å­˜æ¨¡å‹åˆ°ä»¥ä¸‹ä½ç½®ï¼š
    - emergency_save_epoch_XXX.pth (Ctrl+Cæˆ–SIGTERMä¿¡å·)
    - exception_save_epoch_XXX.pth (ç¨‹åºå¼‚å¸¸)
    - exit_save_epoch_XXX.pth (æ­£å¸¸é€€å‡º)
    - latest.pth (æ€»æ˜¯æ›´æ–°ä¸ºæœ€æ–°çŠ¶æ€)
    """
